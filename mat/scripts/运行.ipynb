{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3694d190-e26d-49d6-ad20-dc439f79c016",
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'Exception'>",
     "evalue": "File `'plot_image3.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File `'plot_image3.py'` not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/xpython_846846/2311385877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'plot_image3.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: File `'plot_image3.py'` not found."
     ]
    }
   ],
   "source": [
    "%run plot_image3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348aa0c-135d-45e0-b32e-e41e04722a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env is migrate, scenario is pre-migrate, algo is mappo, exp is single, seed is 1\n",
      "mumu config:  Namespace(add_agent_id=False, add_center_xy=False, add_distance_state=False, add_enemy_action_state=False, add_local_obs=False, add_move_state=False, add_visible_state=False, add_xy_state=False, algorithm_name='mappo', clip_param=0.05, critic_lr=0.0005, cuda=True, cuda_deterministic=True, data_chunk_length=10, dec_actor=False, encode_state=False, entropy_coef=0.01, env_name='migrate', episode_length=200, eval_episodes=32, eval_interval=25, eval_maps=None, experiment_name='single', gae_lambda=0.95, gain=0.01, gamma=0.99, hidden_size=64, huber_delta=10.0, ifi=0.1, layer_N=2, log_interval=5, lr=0.0005, max_grad_norm=0.5, model_dir=None, n_agent=12, n_block=1, n_embd=64, n_eval_rollout_threads=1, n_head=1, n_render_rollout_threads=1, n_rollout_threads=20, n_training_threads=16, num_env_steps=10000000, num_mini_batch=1, opti_eps=1e-05, ppo_epoch=10, recurrent_N=1, render_episodes=5, save_gifs=False, save_interval=100, scenario='pre-migrate', seed=1, share_actor=False, share_policy=True, stacked_frames=1, train_maps=None, use_ReLU=True, use_centralized_V=True, use_clipped_value_loss=True, use_eval=True, use_feature_normalization=True, use_gae=True, use_huber_loss=True, use_linear_lr_decay=False, use_max_grad_norm=True, use_mustalive=True, use_naive_recurrent_policy=False, use_obs_instead_of_state=False, use_orthogonal=True, use_policy_active_masks=False, use_popart=False, use_proper_time_limits=False, use_recurrent_policy=False, use_render=False, use_stacked_frames=False, use_state_agent=False, use_value_active_masks=False, use_valuenorm=True, use_wandb=False, user_name='xxx', value_loss_coef=1, weight_decay=0)\n",
      "choose to use gpu...\n",
      "obs_space:  [Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64)]\n",
      "share_obs_space:  [Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64), Box([900.  10.   0.   0.   0.   0.   0.], [2250.   10.  100.  100.   50.   50. 1000.], (7,), float64)]\n",
      "act_space:  [Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(2)]\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 0/2500 episodes, total num timesteps 4000/10000000, FPS 1457.\n",
      "\n",
      "average_step_rewards is -72.7571029663086.\n",
      "some episodes done, average rewards: -727.5710564134996\n",
      "eval average episode rewards: -728.8141491104252\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 5/2500 episodes, total num timesteps 24000/10000000, FPS 2000.\n",
      "\n",
      "average_step_rewards is -72.6607437133789.\n",
      "some episodes done, average rewards: -726.7438025487818\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 10/2500 episodes, total num timesteps 44000/10000000, FPS 2119.\n",
      "\n",
      "average_step_rewards is -71.91056060791016.\n",
      "some episodes done, average rewards: -723.6677991463255\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 15/2500 episodes, total num timesteps 64000/10000000, FPS 2152.\n",
      "\n",
      "average_step_rewards is -71.52001953125.\n",
      "some episodes done, average rewards: -716.5451340286256\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 20/2500 episodes, total num timesteps 84000/10000000, FPS 2176.\n",
      "\n",
      "average_step_rewards is -70.70214080810547.\n",
      "some episodes done, average rewards: -709.5598503635533\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 25/2500 episodes, total num timesteps 104000/10000000, FPS 2198.\n",
      "\n",
      "average_step_rewards is -69.69664001464844.\n",
      "some episodes done, average rewards: -699.8527361671605\n",
      "eval average episode rewards: -605.7626545797623\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 30/2500 episodes, total num timesteps 124000/10000000, FPS 2203.\n",
      "\n",
      "average_step_rewards is -68.17097473144531.\n",
      "some episodes done, average rewards: -686.5204008515374\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 35/2500 episodes, total num timesteps 144000/10000000, FPS 2212.\n",
      "\n",
      "average_step_rewards is -67.6427001953125.\n",
      "some episodes done, average rewards: -678.6131127337612\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 40/2500 episodes, total num timesteps 164000/10000000, FPS 2216.\n",
      "\n",
      "average_step_rewards is -66.76830291748047.\n",
      "some episodes done, average rewards: -670.3627303303349\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 45/2500 episodes, total num timesteps 184000/10000000, FPS 2218.\n",
      "\n",
      "average_step_rewards is -65.6561279296875.\n",
      "some episodes done, average rewards: -662.2075044950415\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 50/2500 episodes, total num timesteps 204000/10000000, FPS 2222.\n",
      "\n",
      "average_step_rewards is -64.66766357421875.\n",
      "some episodes done, average rewards: -650.4590778514183\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 55/2500 episodes, total num timesteps 224000/10000000, FPS 2224.\n",
      "\n",
      "average_step_rewards is -63.46518325805664.\n",
      "some episodes done, average rewards: -641.304127245285\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 60/2500 episodes, total num timesteps 244000/10000000, FPS 2221.\n",
      "\n",
      "average_step_rewards is -63.64120864868164.\n",
      "some episodes done, average rewards: -635.5504585632908\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 65/2500 episodes, total num timesteps 264000/10000000, FPS 2216.\n",
      "\n",
      "average_step_rewards is -66.05211639404297.\n",
      "some episodes done, average rewards: -648.9963986467048\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 70/2500 episodes, total num timesteps 284000/10000000, FPS 2212.\n",
      "\n",
      "average_step_rewards is -63.74224090576172.\n",
      "some episodes done, average rewards: -642.5101401854585\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 75/2500 episodes, total num timesteps 304000/10000000, FPS 2215.\n",
      "\n",
      "average_step_rewards is -65.14424133300781.\n",
      "some episodes done, average rewards: -644.9277886371267\n",
      "eval average episode rewards: -628.2042750439059\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 80/2500 episodes, total num timesteps 324000/10000000, FPS 2213.\n",
      "\n",
      "average_step_rewards is -66.26280975341797.\n",
      "some episodes done, average rewards: -658.7979727604311\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 85/2500 episodes, total num timesteps 344000/10000000, FPS 2214.\n",
      "\n",
      "average_step_rewards is -66.8945541381836.\n",
      "some episodes done, average rewards: -666.9142201499934\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 90/2500 episodes, total num timesteps 364000/10000000, FPS 2215.\n",
      "\n",
      "average_step_rewards is -67.24866485595703.\n",
      "some episodes done, average rewards: -671.4112987807787\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 95/2500 episodes, total num timesteps 384000/10000000, FPS 2218.\n",
      "\n",
      "average_step_rewards is -67.421142578125.\n",
      "some episodes done, average rewards: -673.6108957988774\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 100/2500 episodes, total num timesteps 404000/10000000, FPS 2220.\n",
      "\n",
      "average_step_rewards is -67.41817474365234.\n",
      "some episodes done, average rewards: -673.6641750226789\n",
      "eval average episode rewards: -662.1615193685282\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 105/2500 episodes, total num timesteps 424000/10000000, FPS 2221.\n",
      "\n",
      "average_step_rewards is -67.36302185058594.\n",
      "some episodes done, average rewards: -673.6975863497726\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 110/2500 episodes, total num timesteps 444000/10000000, FPS 2224.\n",
      "\n",
      "average_step_rewards is -67.34007263183594.\n",
      "some episodes done, average rewards: -673.0947441904552\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 115/2500 episodes, total num timesteps 464000/10000000, FPS 2225.\n",
      "\n",
      "average_step_rewards is -67.30330657958984.\n",
      "some episodes done, average rewards: -673.0130939929194\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 120/2500 episodes, total num timesteps 484000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.22160339355469.\n",
      "some episodes done, average rewards: -672.4032374396207\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 125/2500 episodes, total num timesteps 504000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -67.24415588378906.\n",
      "some episodes done, average rewards: -672.4202957575286\n",
      "eval average episode rewards: -662.1615193685282\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 130/2500 episodes, total num timesteps 524000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.22400665283203.\n",
      "some episodes done, average rewards: -672.1987306512212\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 135/2500 episodes, total num timesteps 544000/10000000, FPS 2224.\n",
      "\n",
      "average_step_rewards is -67.14470672607422.\n",
      "some episodes done, average rewards: -671.4621277314125\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 140/2500 episodes, total num timesteps 564000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.18260192871094.\n",
      "some episodes done, average rewards: -671.7907189605324\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 145/2500 episodes, total num timesteps 584000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.13099670410156.\n",
      "some episodes done, average rewards: -671.3407136328074\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 150/2500 episodes, total num timesteps 604000/10000000, FPS 2228.\n",
      "\n",
      "average_step_rewards is -66.95223236083984.\n",
      "some episodes done, average rewards: -670.0940513930531\n",
      "eval average episode rewards: -662.1615193685282\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 155/2500 episodes, total num timesteps 624000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.05026245117188.\n",
      "some episodes done, average rewards: -670.1686696835816\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 160/2500 episodes, total num timesteps 644000/10000000, FPS 2222.\n",
      "\n",
      "average_step_rewards is -67.19243621826172.\n",
      "some episodes done, average rewards: -671.3193659441458\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 165/2500 episodes, total num timesteps 664000/10000000, FPS 2223.\n",
      "\n",
      "average_step_rewards is -67.19251251220703.\n",
      "some episodes done, average rewards: -671.6404809173499\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 170/2500 episodes, total num timesteps 684000/10000000, FPS 2225.\n",
      "\n",
      "average_step_rewards is -67.1580810546875.\n",
      "some episodes done, average rewards: -672.3951456539717\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 175/2500 episodes, total num timesteps 704000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -67.2117919921875.\n",
      "some episodes done, average rewards: -671.9690256524158\n",
      "eval average episode rewards: -675.3131389615237\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 180/2500 episodes, total num timesteps 724000/10000000, FPS 2225.\n",
      "\n",
      "average_step_rewards is -66.6895751953125.\n",
      "some episodes done, average rewards: -669.5862376220982\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 185/2500 episodes, total num timesteps 744000/10000000, FPS 2225.\n",
      "\n",
      "average_step_rewards is -66.41407012939453.\n",
      "some episodes done, average rewards: -664.5102178249675\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 190/2500 episodes, total num timesteps 764000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -64.85950469970703.\n",
      "some episodes done, average rewards: -656.2737987396922\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 195/2500 episodes, total num timesteps 784000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -64.6695556640625.\n",
      "some episodes done, average rewards: -646.7589476438549\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 200/2500 episodes, total num timesteps 804000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -63.96783447265625.\n",
      "some episodes done, average rewards: -642.5175294801563\n",
      "eval average episode rewards: -607.9280088017797\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 205/2500 episodes, total num timesteps 824000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -62.919952392578125.\n",
      "some episodes done, average rewards: -633.3027215098848\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 210/2500 episodes, total num timesteps 844000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -62.750511169433594.\n",
      "some episodes done, average rewards: -628.5820542891863\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 215/2500 episodes, total num timesteps 864000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -62.49026107788086.\n",
      "some episodes done, average rewards: -626.1923508098901\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 220/2500 episodes, total num timesteps 884000/10000000, FPS 2226.\n",
      "\n",
      "average_step_rewards is -62.51478576660156.\n",
      "some episodes done, average rewards: -624.7871609522372\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 225/2500 episodes, total num timesteps 904000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -62.089874267578125.\n",
      "some episodes done, average rewards: -622.3842656655814\n",
      "eval average episode rewards: -607.516639346265\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 230/2500 episodes, total num timesteps 924000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -61.84286880493164.\n",
      "some episodes done, average rewards: -618.5461840810867\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 235/2500 episodes, total num timesteps 944000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -61.76955032348633.\n",
      "some episodes done, average rewards: -618.4109030120462\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 240/2500 episodes, total num timesteps 964000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -61.60675048828125.\n",
      "some episodes done, average rewards: -616.5883028033853\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 245/2500 episodes, total num timesteps 984000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -61.479454040527344.\n",
      "some episodes done, average rewards: -615.1911594119346\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 250/2500 episodes, total num timesteps 1004000/10000000, FPS 2228.\n",
      "\n",
      "average_step_rewards is -61.39785385131836.\n",
      "some episodes done, average rewards: -614.1272108279752\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 255/2500 episodes, total num timesteps 1024000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.29091262817383.\n",
      "some episodes done, average rewards: -613.2194041902048\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 260/2500 episodes, total num timesteps 1044000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.26961898803711.\n",
      "some episodes done, average rewards: -612.9421003664984\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 265/2500 episodes, total num timesteps 1064000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.23164749145508.\n",
      "some episodes done, average rewards: -612.399660652315\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 270/2500 episodes, total num timesteps 1084000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.41511535644531.\n",
      "some episodes done, average rewards: -613.9519901529534\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 275/2500 episodes, total num timesteps 1104000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.405479431152344.\n",
      "some episodes done, average rewards: -614.3064655429746\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 280/2500 episodes, total num timesteps 1124000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.51434326171875.\n",
      "some episodes done, average rewards: -615.5666607315451\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 285/2500 episodes, total num timesteps 1144000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.30580139160156.\n",
      "some episodes done, average rewards: -616.2831111966436\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 290/2500 episodes, total num timesteps 1164000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.44842529296875.\n",
      "some episodes done, average rewards: -614.3234117311983\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 295/2500 episodes, total num timesteps 1184000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.56474304199219.\n",
      "some episodes done, average rewards: -615.0235975470689\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 300/2500 episodes, total num timesteps 1204000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.87484359741211.\n",
      "some episodes done, average rewards: -619.0476398281135\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 305/2500 episodes, total num timesteps 1224000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -62.03022384643555.\n",
      "some episodes done, average rewards: -618.4439256628249\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 310/2500 episodes, total num timesteps 1244000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.705562591552734.\n",
      "some episodes done, average rewards: -618.4421431585429\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 315/2500 episodes, total num timesteps 1264000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.63798522949219.\n",
      "some episodes done, average rewards: -617.0948302539585\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 320/2500 episodes, total num timesteps 1284000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.672760009765625.\n",
      "some episodes done, average rewards: -616.6629437000234\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 325/2500 episodes, total num timesteps 1304000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.454769134521484.\n",
      "some episodes done, average rewards: -615.3451180464186\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 330/2500 episodes, total num timesteps 1324000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.342533111572266.\n",
      "some episodes done, average rewards: -614.9210986718533\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 335/2500 episodes, total num timesteps 1344000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.08861541748047.\n",
      "some episodes done, average rewards: -613.3918875861403\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 340/2500 episodes, total num timesteps 1364000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -60.94824981689453.\n",
      "some episodes done, average rewards: -610.3829624385343\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 345/2500 episodes, total num timesteps 1384000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.12767028808594.\n",
      "some episodes done, average rewards: -611.0258150172043\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 350/2500 episodes, total num timesteps 1404000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.07323455810547.\n",
      "some episodes done, average rewards: -611.0039061829045\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 355/2500 episodes, total num timesteps 1424000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.01704025268555.\n",
      "some episodes done, average rewards: -610.354203587515\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 360/2500 episodes, total num timesteps 1444000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.00887680053711.\n",
      "some episodes done, average rewards: -609.7320704221523\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 365/2500 episodes, total num timesteps 1464000/10000000, FPS 2233.\n",
      "\n",
      "average_step_rewards is -61.041343688964844.\n",
      "some episodes done, average rewards: -610.3290555583777\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 370/2500 episodes, total num timesteps 1484000/10000000, FPS 2234.\n",
      "\n",
      "average_step_rewards is -61.06321716308594.\n",
      "some episodes done, average rewards: -610.5689486759634\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 375/2500 episodes, total num timesteps 1504000/10000000, FPS 2233.\n",
      "\n",
      "average_step_rewards is -60.94453811645508.\n",
      "some episodes done, average rewards: -609.8052176612122\n",
      "eval average episode rewards: -605.7626545797623\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 380/2500 episodes, total num timesteps 1524000/10000000, FPS 2233.\n",
      "\n",
      "average_step_rewards is -60.79570388793945.\n",
      "some episodes done, average rewards: -608.0685848231509\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 385/2500 episodes, total num timesteps 1544000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -60.890541076660156.\n",
      "some episodes done, average rewards: -607.8505166964322\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 390/2500 episodes, total num timesteps 1564000/10000000, FPS 2233.\n",
      "\n",
      "average_step_rewards is -60.73256301879883.\n",
      "some episodes done, average rewards: -607.8544201878935\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 395/2500 episodes, total num timesteps 1584000/10000000, FPS 2233.\n",
      "\n",
      "average_step_rewards is -61.28965759277344.\n",
      "some episodes done, average rewards: -612.51141547035\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 400/2500 episodes, total num timesteps 1604000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -60.72193908691406.\n",
      "some episodes done, average rewards: -608.936110720751\n",
      "eval average episode rewards: -605.7626545797623\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 405/2500 episodes, total num timesteps 1624000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -60.84076690673828.\n",
      "some episodes done, average rewards: -608.2943277377693\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 410/2500 episodes, total num timesteps 1644000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -60.881866455078125.\n",
      "some episodes done, average rewards: -608.2355655537409\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 415/2500 episodes, total num timesteps 1664000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -60.856563568115234.\n",
      "some episodes done, average rewards: -608.3722064973969\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 420/2500 episodes, total num timesteps 1684000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.04291534423828.\n",
      "some episodes done, average rewards: -609.7318660475266\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 425/2500 episodes, total num timesteps 1704000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.173770904541016.\n",
      "some episodes done, average rewards: -611.355145645691\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 430/2500 episodes, total num timesteps 1724000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -61.24040222167969.\n",
      "some episodes done, average rewards: -611.7988624219964\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 435/2500 episodes, total num timesteps 1744000/10000000, FPS 2228.\n",
      "\n",
      "average_step_rewards is -60.86014175415039.\n",
      "some episodes done, average rewards: -609.53197692548\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 440/2500 episodes, total num timesteps 1764000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -60.873863220214844.\n",
      "some episodes done, average rewards: -609.3962936033406\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 445/2500 episodes, total num timesteps 1784000/10000000, FPS 2229.\n",
      "\n",
      "average_step_rewards is -60.807655334472656.\n",
      "some episodes done, average rewards: -608.9872196428838\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 450/2500 episodes, total num timesteps 1804000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -60.962677001953125.\n",
      "some episodes done, average rewards: -609.4773331955902\n",
      "eval average episode rewards: -608.7181956294755\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 455/2500 episodes, total num timesteps 1824000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -60.86756134033203.\n",
      "some episodes done, average rewards: -609.9416445711088\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 460/2500 episodes, total num timesteps 1844000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.09343719482422.\n",
      "some episodes done, average rewards: -609.3908726528435\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 465/2500 episodes, total num timesteps 1864000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.43136978149414.\n",
      "some episodes done, average rewards: -611.6950169503729\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 470/2500 episodes, total num timesteps 1884000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.28960037231445.\n",
      "some episodes done, average rewards: -613.80172765874\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 475/2500 episodes, total num timesteps 1904000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.09505081176758.\n",
      "some episodes done, average rewards: -611.3575030596156\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 480/2500 episodes, total num timesteps 1924000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.049102783203125.\n",
      "some episodes done, average rewards: -610.6274622992746\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 485/2500 episodes, total num timesteps 1944000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.0404052734375.\n",
      "some episodes done, average rewards: -610.4796593884905\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 490/2500 episodes, total num timesteps 1964000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.05912399291992.\n",
      "some episodes done, average rewards: -610.4691342014411\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 495/2500 episodes, total num timesteps 1984000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.0396614074707.\n",
      "some episodes done, average rewards: -610.3872469582645\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 500/2500 episodes, total num timesteps 2004000/10000000, FPS 2232.\n",
      "\n",
      "average_step_rewards is -61.03508377075195.\n",
      "some episodes done, average rewards: -610.3472949429937\n",
      "eval average episode rewards: -610.5037755459375\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 505/2500 episodes, total num timesteps 2024000/10000000, FPS 2231.\n",
      "\n",
      "average_step_rewards is -61.02589416503906.\n",
      "some episodes done, average rewards: -610.3776614723615\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 510/2500 episodes, total num timesteps 2044000/10000000, FPS 2230.\n",
      "\n",
      "average_step_rewards is -61.06833267211914.\n",
      "some episodes done, average rewards: -610.6962320553379\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 515/2500 episodes, total num timesteps 2064000/10000000, FPS 2228.\n",
      "\n",
      "average_step_rewards is -61.01535415649414.\n",
      "some episodes done, average rewards: -610.4345280140595\n",
      "\n",
      " Scenario pre-migrate Algo mappo Exp single updates 520/2500 episodes, total num timesteps 2084000/10000000, FPS 2227.\n",
      "\n",
      "average_step_rewards is -61.02677536010742.\n",
      "some episodes done, average rewards: -610.2735382012233\n"
     ]
    }
   ],
   "source": [
    "!bash train_migrate.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d30794-80c8-4a3d-ae36-9f0a29bc6368",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### !jupyter labextension develop . --overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d1eb4-c954-4d8e-8569-66a543dc8e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
